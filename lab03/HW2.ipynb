{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9vuAw7AZdbv"
   },
   "source": [
    "Реализуйте алгоритм SAC для среды lunar lander"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VCWEy3pfMvHA",
    "ExecuteTime": {
     "end_time": "2025-04-23T14:03:01.430007Z",
     "start_time": "2025-04-23T14:02:59.529453Z"
    }
   },
   "source": [
    "!pip install swig\n",
    "!pip install \"gymnasium[box2d]\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /home/lxuser/.pyenv/versions/3.12.9/lib/python3.12/site-packages (4.3.1)\r\n",
      "Requirement already satisfied: gymnasium[box2d] in /home/lxuser/.pyenv/versions/3.12.9/lib/python3.12/site-packages (1.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/lxuser/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from gymnasium[box2d]) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/lxuser/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from gymnasium[box2d]) (3.1.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/lxuser/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from gymnasium[box2d]) (4.12.2)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/lxuser/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from gymnasium[box2d]) (0.0.4)\r\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /home/lxuser/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from gymnasium[box2d]) (2.3.5)\r\n",
      "Requirement already satisfied: pygame>=2.1.3 in /home/lxuser/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from gymnasium[box2d]) (2.6.1)\r\n",
      "Requirement already satisfied: swig==4.* in /home/lxuser/.pyenv/versions/3.12.9/lib/python3.12/site-packages (from gymnasium[box2d]) (4.3.1)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5jSxwGXvtAWp",
    "ExecuteTime": {
     "end_time": "2025-04-23T14:03:02.769644Z",
     "start_time": "2025-04-23T14:03:01.434461Z"
    }
   },
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gBd1ntZM6uaH",
    "ExecuteTime": {
     "end_time": "2025-04-23T14:03:02.843425Z",
     "start_time": "2025-04-23T14:03:02.840954Z"
    }
   },
   "source": [
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "ALPHA = 0.2\n",
    "ACTOR_LR = 3e-4\n",
    "CRITIC_LR = 3e-4\n",
    "REPLAY_SIZE = 100000\n",
    "BATCH_SIZE = 256\n",
    "START_STEPS = 10000\n",
    "TOTAL_STEPS = 200000\n",
    "UPDATE_AFTER = 1000\n",
    "UPDATE_EVERY = 50"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T14:03:02.970011Z",
     "start_time": "2025-04-23T14:03:02.884984Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4VVfs61H6xlX",
    "ExecuteTime": {
     "end_time": "2025-04-23T14:03:03.026014Z",
     "start_time": "2025-04-23T14:03:03.014247Z"
    }
   },
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, action_low, action_high):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "        )\n",
    "        self.mu_layer = nn.Linear(256, act_dim)\n",
    "        self.log_std_layer = nn.Linear(256, act_dim)\n",
    "\n",
    "        self.action_low = torch.tensor(action_low, dtype=torch.float32).to(device)\n",
    "        self.action_high = torch.tensor(action_high, dtype=torch.float32).to(device)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = F.relu(self.net(obs))\n",
    "        mean, std = self.mu_layer(x), torch.clamp(self.log_std_layer(x), -20, 2).exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "\n",
    "        x_t = normal.rsample()\n",
    "        y_t = torch.tanh(x_t)\n",
    "        action = y_t * (action_high - action_low) / 2.0 + (action_low + action_high) / 2.0\n",
    "\n",
    "        log_prob = normal.log_prob(x_t)\n",
    "        log_prob -= torch.log((1 - y_t.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def get_action(self, obs, deterministic=False):\n",
    "        # Напишите функцию, которая возвращает только действие. Если deterministic True, то действие не семплируется, а просто берётся mean (его всё ещё надо предобразовать)\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "            x = self.net(obs_tensor)\n",
    "            mean = self.mu_layer(x)\n",
    "\n",
    "            if deterministic:\n",
    "                y_t = torch.tanh(mean)\n",
    "                action = y_t * (self.action_high - self.action_low) / 2 + (self.action_high + self.action_low) / 2\n",
    "            else:\n",
    "                log_std = self.log_std_layer(x)\n",
    "                log_std = torch.clamp(log_std, -20, 2)\n",
    "                std = log_std.exp()\n",
    "                dist = Normal(mean, std)\n",
    "                x_t = dist.rsample()\n",
    "                y_t = torch.tanh(x_t)\n",
    "                action = y_t * (self.action_high - self.action_low) / 2 + (self.action_high + self.action_low) / 2\n",
    "\n",
    "            return action.squeeze(0).cpu().numpy()\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q8P22VHh62j0",
    "ExecuteTime": {
     "end_time": "2025-04-23T14:03:03.072683Z",
     "start_time": "2025-04-23T14:03:03.066989Z"
    }
   },
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.q1(x), self.q2(x)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yt5KVfiq65_V",
    "ExecuteTime": {
     "end_time": "2025-04-23T14:03:03.123336Z",
     "start_time": "2025-04-23T14:03:03.116831Z"
    }
   },
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.buffer.append(tuple(args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.float32).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).unsqueeze(1).to(device),\n",
    "            torch.tensor(next_states, dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AZ5gtdcF69dS",
    "ExecuteTime": {
     "end_time": "2025-04-23T14:03:04.168034Z",
     "start_time": "2025-04-23T14:03:03.167371Z"
    }
   },
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v3\")\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "action_low, action_high = float(env.action_space.low[0]), float(env.action_space.high[0])\n",
    "\n",
    "actor = Actor(obs_dim, act_dim, action_low, action_high).to(device)\n",
    "critic = Critic(obs_dim, act_dim).to(device)\n",
    "critic_target = Critic(obs_dim, act_dim).to(device)\n",
    "critic_target.load_state_dict(critic.state_dict())\n",
    "\n",
    "actor_opt = optim.Adam(actor.parameters(), lr=ACTOR_LR)\n",
    "critic_opt = optim.Adam(critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "target_entropy = -torch.prod(torch.Tensor(env.action_space.shape).to(device)).item()\n",
    "log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "alpha_optim = optim.Adam([log_alpha], lr=ACTOR_LR)\n",
    "ALPHA = log_alpha.exp().item()\n",
    "\n",
    "replay = ReplayBuffer(REPLAY_SIZE)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "episode_return, episode_len = 0, 0"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "an3ShfO27FWv",
    "ExecuteTime": {
     "end_time": "2025-04-23T14:43:32.234393Z",
     "start_time": "2025-04-23T14:03:04.201355Z"
    }
   },
   "source": [
    "for step in range(TOTAL_STEPS):\n",
    "    if step < START_STEPS:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = actor.get_action(obs)\n",
    "        action = np.clip(action, action_low, action_high)\n",
    "\n",
    "    next_obs, rew, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    replay.add(obs, action, rew, next_obs, done)\n",
    "\n",
    "    obs = next_obs\n",
    "    episode_return += rew\n",
    "    episode_len += 1\n",
    "\n",
    "    if done:\n",
    "        obs, _ = env.reset()\n",
    "        print(f\"Step: {step}, Return: {episode_return:.2f}, Len: {episode_len}\")\n",
    "        episode_return, episode_len = 0, 0\n",
    "\n",
    "    if step >= UPDATE_AFTER and step % UPDATE_EVERY == 0:\n",
    "        for _ in range(UPDATE_EVERY):\n",
    "            batch = replay.sample(BATCH_SIZE)\n",
    "            state_batch, action_batch, reward_batch, next_state_batch, done_batch = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                next_action, next_log_prob = actor(next_state_batch)\n",
    "                q1_next, q2_next = critic_target(next_state_batch, next_action)\n",
    "                q_next = torch.min(q1_next, q2_next) - ALPHA * next_log_prob\n",
    "                target_q = reward_batch + (1 - done_batch) * GAMMA * q_next\n",
    "\n",
    "            q1, q2 = critic(state_batch, action_batch)\n",
    "            critic_loss = F.mse_loss(q1, target_q) + F.mse_loss(q2, target_q)\n",
    "\n",
    "            critic_opt.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_opt.step()\n",
    "\n",
    "            actions, log_probs = actor(state_batch)\n",
    "            q1, q2 = critic(state_batch, actions)\n",
    "            q = torch.min(q1, q2)\n",
    "            actor_loss = (ALPHA * log_probs - q).mean()\n",
    "\n",
    "            actor_opt.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_opt.step()\n",
    "\n",
    "            alpha_loss = -(log_alpha * (log_probs.detach() + target_entropy)).mean()\n",
    "            alpha_optim.zero_grad()\n",
    "            alpha_loss.backward()\n",
    "            alpha_optim.step()\n",
    "            ALPHA = log_alpha.exp().item()\n",
    "\n",
    "            for target_param, source_param in zip(critic_target.parameters(), critic.parameters()):\n",
    "                target_param.data.copy_(target_param.data * (1.0 - TAU) + source_param.data * TAU)\n",
    "\n",
    "        env.close()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 83, Return: -102.50, Len: 84\n",
      "Step: 190, Return: -308.40, Len: 107\n",
      "Step: 325, Return: -186.76, Len: 135\n",
      "Step: 446, Return: -269.16, Len: 121\n",
      "Step: 523, Return: -61.73, Len: 77\n",
      "Step: 609, Return: -383.28, Len: 86\n",
      "Step: 733, Return: -127.10, Len: 124\n",
      "Step: 830, Return: -163.79, Len: 97\n",
      "Step: 919, Return: -147.09, Len: 89\n",
      "Step: 1043, Return: -102.68, Len: 124\n",
      "Step: 1149, Return: -224.89, Len: 106\n",
      "Step: 1262, Return: -103.07, Len: 113\n",
      "Step: 1374, Return: -210.87, Len: 112\n",
      "Step: 1518, Return: -235.68, Len: 144\n",
      "Step: 1768, Return: -163.81, Len: 250\n",
      "Step: 1857, Return: -93.49, Len: 89\n",
      "Step: 1965, Return: -73.42, Len: 108\n",
      "Step: 2073, Return: -236.21, Len: 108\n",
      "Step: 2182, Return: -443.43, Len: 109\n",
      "Step: 2297, Return: -180.26, Len: 115\n",
      "Step: 2427, Return: -244.69, Len: 130\n",
      "Step: 2504, Return: -70.68, Len: 77\n",
      "Step: 2643, Return: -334.98, Len: 139\n",
      "Step: 2785, Return: -146.11, Len: 142\n",
      "Step: 2879, Return: -331.85, Len: 94\n",
      "Step: 2959, Return: -404.88, Len: 80\n",
      "Step: 3044, Return: -66.87, Len: 85\n",
      "Step: 3129, Return: -350.33, Len: 85\n",
      "Step: 3193, Return: -106.27, Len: 64\n",
      "Step: 3328, Return: -203.54, Len: 135\n",
      "Step: 3418, Return: -193.56, Len: 90\n",
      "Step: 3598, Return: -109.05, Len: 180\n",
      "Step: 3720, Return: -291.05, Len: 122\n",
      "Step: 3817, Return: -98.31, Len: 97\n",
      "Step: 3903, Return: -41.16, Len: 86\n",
      "Step: 4026, Return: -196.74, Len: 123\n",
      "Step: 4105, Return: -477.34, Len: 79\n",
      "Step: 4281, Return: -259.20, Len: 176\n",
      "Step: 4437, Return: -175.26, Len: 156\n",
      "Step: 4539, Return: -231.89, Len: 102\n",
      "Step: 4627, Return: -251.93, Len: 88\n",
      "Step: 4708, Return: -57.75, Len: 81\n",
      "Step: 4813, Return: -232.75, Len: 105\n",
      "Step: 4966, Return: -137.08, Len: 153\n",
      "Step: 5076, Return: -85.47, Len: 110\n",
      "Step: 5165, Return: -283.92, Len: 89\n",
      "Step: 5262, Return: -93.89, Len: 97\n",
      "Step: 5353, Return: -118.37, Len: 91\n",
      "Step: 5453, Return: -395.15, Len: 100\n",
      "Step: 5542, Return: -369.89, Len: 89\n",
      "Step: 5639, Return: -319.04, Len: 97\n",
      "Step: 5744, Return: -134.22, Len: 105\n",
      "Step: 5878, Return: -276.77, Len: 134\n",
      "Step: 5958, Return: -288.07, Len: 80\n",
      "Step: 6046, Return: -312.49, Len: 88\n",
      "Step: 6166, Return: -9.43, Len: 120\n",
      "Step: 6325, Return: -105.08, Len: 159\n",
      "Step: 6450, Return: -117.66, Len: 125\n",
      "Step: 6520, Return: -60.67, Len: 70\n",
      "Step: 6612, Return: -60.29, Len: 92\n",
      "Step: 6706, Return: -433.93, Len: 94\n",
      "Step: 6840, Return: -220.38, Len: 134\n",
      "Step: 6913, Return: -157.30, Len: 73\n",
      "Step: 7019, Return: -378.76, Len: 106\n",
      "Step: 7086, Return: -101.09, Len: 67\n",
      "Step: 7170, Return: -190.18, Len: 84\n",
      "Step: 7296, Return: -226.39, Len: 126\n",
      "Step: 7384, Return: -354.26, Len: 88\n",
      "Step: 7486, Return: -236.66, Len: 102\n",
      "Step: 7626, Return: -140.02, Len: 140\n",
      "Step: 7755, Return: -410.52, Len: 129\n",
      "Step: 7832, Return: -106.92, Len: 77\n",
      "Step: 8003, Return: -475.76, Len: 171\n",
      "Step: 8096, Return: -97.96, Len: 93\n",
      "Step: 8161, Return: -120.64, Len: 65\n",
      "Step: 8239, Return: -106.36, Len: 78\n",
      "Step: 8313, Return: -14.34, Len: 74\n",
      "Step: 8440, Return: -259.59, Len: 127\n",
      "Step: 8526, Return: -152.27, Len: 86\n",
      "Step: 8626, Return: -92.23, Len: 100\n",
      "Step: 8717, Return: -332.06, Len: 91\n",
      "Step: 8796, Return: -186.70, Len: 79\n",
      "Step: 8896, Return: -283.93, Len: 100\n",
      "Step: 8992, Return: -45.07, Len: 96\n",
      "Step: 9093, Return: -124.29, Len: 101\n",
      "Step: 9211, Return: -403.18, Len: 118\n",
      "Step: 9313, Return: -279.18, Len: 102\n",
      "Step: 9418, Return: -416.73, Len: 105\n",
      "Step: 9581, Return: -221.37, Len: 163\n",
      "Step: 9713, Return: -356.56, Len: 132\n",
      "Step: 9812, Return: -261.73, Len: 99\n",
      "Step: 9891, Return: -100.19, Len: 79\n",
      "Step: 9990, Return: -90.65, Len: 99\n",
      "Step: 10990, Return: -55.86, Len: 1000\n",
      "Step: 11368, Return: -108.17, Len: 378\n",
      "Step: 11771, Return: -352.22, Len: 403\n",
      "Step: 12185, Return: 244.05, Len: 414\n",
      "Step: 12570, Return: 238.46, Len: 385\n",
      "Step: 12820, Return: 14.19, Len: 250\n",
      "Step: 13092, Return: -330.76, Len: 272\n",
      "Step: 13260, Return: -45.83, Len: 168\n",
      "Step: 13556, Return: -108.43, Len: 296\n",
      "Step: 13814, Return: -17.16, Len: 258\n",
      "Step: 14020, Return: -54.88, Len: 206\n",
      "Step: 14320, Return: -37.46, Len: 300\n",
      "Step: 14585, Return: -246.05, Len: 265\n",
      "Step: 14905, Return: -28.65, Len: 320\n",
      "Step: 15098, Return: -41.63, Len: 193\n",
      "Step: 15359, Return: -48.83, Len: 261\n",
      "Step: 15759, Return: -44.74, Len: 400\n",
      "Step: 16200, Return: -120.07, Len: 441\n",
      "Step: 16839, Return: -175.69, Len: 639\n",
      "Step: 17079, Return: -36.18, Len: 240\n",
      "Step: 17417, Return: -96.74, Len: 338\n",
      "Step: 17900, Return: -163.09, Len: 483\n",
      "Step: 18645, Return: -225.18, Len: 745\n",
      "Step: 19460, Return: -192.60, Len: 815\n",
      "Step: 20392, Return: -231.35, Len: 932\n",
      "Step: 20788, Return: 185.15, Len: 396\n",
      "Step: 20918, Return: -163.52, Len: 130\n",
      "Step: 21261, Return: -336.13, Len: 343\n",
      "Step: 21659, Return: -140.81, Len: 398\n",
      "Step: 22204, Return: -212.89, Len: 545\n",
      "Step: 22419, Return: 21.88, Len: 215\n",
      "Step: 22715, Return: -81.89, Len: 296\n",
      "Step: 23715, Return: -57.87, Len: 1000\n",
      "Step: 23975, Return: -11.39, Len: 260\n",
      "Step: 24378, Return: -51.12, Len: 403\n",
      "Step: 24645, Return: 17.05, Len: 267\n",
      "Step: 25348, Return: -375.52, Len: 703\n",
      "Step: 25641, Return: -39.67, Len: 293\n",
      "Step: 25893, Return: 3.67, Len: 252\n",
      "Step: 26337, Return: -154.60, Len: 444\n",
      "Step: 26538, Return: 69.78, Len: 201\n",
      "Step: 26760, Return: -250.06, Len: 222\n",
      "Step: 27427, Return: -336.37, Len: 667\n",
      "Step: 28189, Return: -118.32, Len: 762\n",
      "Step: 28537, Return: -3.18, Len: 348\n",
      "Step: 28773, Return: 15.06, Len: 236\n",
      "Step: 29277, Return: -237.27, Len: 504\n",
      "Step: 29662, Return: 174.37, Len: 385\n",
      "Step: 30030, Return: 0.62, Len: 368\n",
      "Step: 30363, Return: 222.45, Len: 333\n",
      "Step: 30598, Return: 260.40, Len: 235\n",
      "Step: 30923, Return: -89.23, Len: 325\n",
      "Step: 31185, Return: -50.12, Len: 262\n",
      "Step: 31632, Return: 260.85, Len: 447\n",
      "Step: 32185, Return: -636.07, Len: 553\n",
      "Step: 32707, Return: 210.82, Len: 522\n",
      "Step: 33482, Return: 115.73, Len: 775\n",
      "Step: 34482, Return: 65.33, Len: 1000\n",
      "Step: 34826, Return: -85.45, Len: 344\n",
      "Step: 35674, Return: 156.07, Len: 848\n",
      "Step: 36209, Return: -13.95, Len: 535\n",
      "Step: 36513, Return: -15.30, Len: 304\n",
      "Step: 36843, Return: -46.56, Len: 330\n",
      "Step: 37444, Return: 168.70, Len: 601\n",
      "Step: 38084, Return: -89.93, Len: 640\n",
      "Step: 39084, Return: -104.84, Len: 1000\n",
      "Step: 39222, Return: -20.60, Len: 138\n",
      "Step: 39366, Return: 43.17, Len: 144\n",
      "Step: 39477, Return: 14.98, Len: 111\n",
      "Step: 39628, Return: 22.78, Len: 151\n",
      "Step: 40628, Return: 26.92, Len: 1000\n",
      "Step: 41628, Return: 4.69, Len: 1000\n",
      "Step: 41796, Return: 7.96, Len: 168\n",
      "Step: 42480, Return: 230.70, Len: 684\n",
      "Step: 43480, Return: -83.18, Len: 1000\n",
      "Step: 44480, Return: -57.22, Len: 1000\n",
      "Step: 44575, Return: -11.62, Len: 95\n",
      "Step: 45502, Return: 97.36, Len: 927\n",
      "Step: 46189, Return: 133.98, Len: 687\n",
      "Step: 47189, Return: -53.88, Len: 1000\n",
      "Step: 48189, Return: 9.90, Len: 1000\n",
      "Step: 48562, Return: 266.91, Len: 373\n",
      "Step: 49562, Return: -72.61, Len: 1000\n",
      "Step: 50562, Return: 2.98, Len: 1000\n",
      "Step: 51562, Return: 4.75, Len: 1000\n",
      "Step: 52562, Return: -51.22, Len: 1000\n",
      "Step: 53562, Return: -8.36, Len: 1000\n",
      "Step: 54562, Return: -54.79, Len: 1000\n",
      "Step: 55562, Return: -88.73, Len: 1000\n",
      "Step: 56248, Return: -239.42, Len: 686\n",
      "Step: 57248, Return: -107.98, Len: 1000\n",
      "Step: 57775, Return: -168.41, Len: 527\n",
      "Step: 58172, Return: 147.05, Len: 397\n",
      "Step: 58381, Return: 217.68, Len: 209\n",
      "Step: 58590, Return: 248.28, Len: 209\n",
      "Step: 59431, Return: -462.02, Len: 841\n",
      "Step: 60139, Return: -227.86, Len: 708\n",
      "Step: 61071, Return: 130.88, Len: 932\n",
      "Step: 61511, Return: -73.55, Len: 440\n",
      "Step: 61653, Return: 14.01, Len: 142\n",
      "Step: 62396, Return: 125.25, Len: 743\n",
      "Step: 62795, Return: -187.20, Len: 399\n",
      "Step: 63795, Return: -126.76, Len: 1000\n",
      "Step: 63920, Return: -16.79, Len: 125\n",
      "Step: 64685, Return: -224.09, Len: 765\n",
      "Step: 65210, Return: 211.62, Len: 525\n",
      "Step: 66210, Return: -81.17, Len: 1000\n",
      "Step: 66615, Return: 185.58, Len: 405\n",
      "Step: 66863, Return: 185.47, Len: 248\n",
      "Step: 67431, Return: -202.62, Len: 568\n",
      "Step: 68090, Return: 212.07, Len: 659\n",
      "Step: 68423, Return: 238.83, Len: 333\n",
      "Step: 68822, Return: -36.36, Len: 399\n",
      "Step: 69815, Return: -275.20, Len: 993\n",
      "Step: 70303, Return: 153.20, Len: 488\n",
      "Step: 70951, Return: -181.09, Len: 648\n",
      "Step: 71616, Return: 219.90, Len: 665\n",
      "Step: 72541, Return: 178.47, Len: 925\n",
      "Step: 73541, Return: -29.49, Len: 1000\n",
      "Step: 73747, Return: 1.03, Len: 206\n",
      "Step: 73950, Return: 18.33, Len: 203\n",
      "Step: 74306, Return: 199.21, Len: 356\n",
      "Step: 74530, Return: -66.75, Len: 224\n",
      "Step: 75302, Return: 78.90, Len: 772\n",
      "Step: 75520, Return: 3.06, Len: 218\n",
      "Step: 76056, Return: 160.46, Len: 536\n",
      "Step: 76396, Return: 263.73, Len: 340\n",
      "Step: 76912, Return: 180.35, Len: 516\n",
      "Step: 77810, Return: 102.67, Len: 898\n",
      "Step: 78794, Return: 117.97, Len: 984\n",
      "Step: 79026, Return: 282.92, Len: 232\n",
      "Step: 79449, Return: 182.62, Len: 423\n",
      "Step: 80048, Return: 170.25, Len: 599\n",
      "Step: 80413, Return: 225.87, Len: 365\n",
      "Step: 81413, Return: -29.13, Len: 1000\n",
      "Step: 81886, Return: 205.56, Len: 473\n",
      "Step: 82253, Return: 229.31, Len: 367\n",
      "Step: 82527, Return: 253.54, Len: 274\n",
      "Step: 82794, Return: 31.19, Len: 267\n",
      "Step: 83022, Return: 270.26, Len: 228\n",
      "Step: 83663, Return: 214.17, Len: 641\n",
      "Step: 84177, Return: 242.43, Len: 514\n",
      "Step: 84797, Return: -40.82, Len: 620\n",
      "Step: 85221, Return: 241.83, Len: 424\n",
      "Step: 85613, Return: 251.19, Len: 392\n",
      "Step: 86580, Return: 123.60, Len: 967\n",
      "Step: 86887, Return: -38.77, Len: 307\n",
      "Step: 87371, Return: 243.93, Len: 484\n",
      "Step: 87575, Return: -118.09, Len: 204\n",
      "Step: 88028, Return: 239.48, Len: 453\n",
      "Step: 88375, Return: 253.52, Len: 347\n",
      "Step: 88550, Return: -9.26, Len: 175\n",
      "Step: 89019, Return: 254.67, Len: 469\n",
      "Step: 89211, Return: 8.08, Len: 192\n",
      "Step: 89659, Return: -365.18, Len: 448\n",
      "Step: 89920, Return: -129.27, Len: 261\n",
      "Step: 90217, Return: 248.80, Len: 297\n",
      "Step: 90531, Return: 217.78, Len: 314\n",
      "Step: 91022, Return: -316.23, Len: 491\n",
      "Step: 91358, Return: -241.12, Len: 336\n",
      "Step: 91527, Return: 252.05, Len: 169\n",
      "Step: 91666, Return: -19.38, Len: 139\n",
      "Step: 92103, Return: 248.34, Len: 437\n",
      "Step: 92322, Return: -169.84, Len: 219\n",
      "Step: 92492, Return: -93.08, Len: 170\n",
      "Step: 92773, Return: 203.05, Len: 281\n",
      "Step: 93021, Return: -417.13, Len: 248\n",
      "Step: 93382, Return: 184.82, Len: 361\n",
      "Step: 93724, Return: 244.78, Len: 342\n",
      "Step: 94234, Return: 210.76, Len: 510\n",
      "Step: 95030, Return: 99.32, Len: 796\n",
      "Step: 95276, Return: 229.31, Len: 246\n",
      "Step: 95580, Return: 280.29, Len: 304\n",
      "Step: 95793, Return: -36.22, Len: 213\n",
      "Step: 96181, Return: 307.64, Len: 388\n",
      "Step: 96528, Return: 172.21, Len: 347\n",
      "Step: 96814, Return: 188.73, Len: 286\n",
      "Step: 97146, Return: 260.60, Len: 332\n",
      "Step: 97501, Return: 227.43, Len: 355\n",
      "Step: 97724, Return: 266.54, Len: 223\n",
      "Step: 98440, Return: -190.78, Len: 716\n",
      "Step: 98735, Return: 190.67, Len: 295\n",
      "Step: 98962, Return: 254.25, Len: 227\n",
      "Step: 99474, Return: 191.66, Len: 512\n",
      "Step: 99661, Return: 255.96, Len: 187\n",
      "Step: 99925, Return: 243.56, Len: 264\n",
      "Step: 100225, Return: 230.39, Len: 300\n",
      "Step: 100436, Return: 249.05, Len: 211\n",
      "Step: 100788, Return: 200.75, Len: 352\n",
      "Step: 101328, Return: 213.70, Len: 540\n",
      "Step: 101495, Return: 238.65, Len: 167\n",
      "Step: 101869, Return: 243.26, Len: 374\n",
      "Step: 102227, Return: 176.62, Len: 358\n",
      "Step: 102385, Return: -14.79, Len: 158\n",
      "Step: 102518, Return: -43.41, Len: 133\n",
      "Step: 102881, Return: 222.87, Len: 363\n",
      "Step: 103409, Return: 204.58, Len: 528\n",
      "Step: 103801, Return: 203.14, Len: 392\n",
      "Step: 104236, Return: 249.88, Len: 435\n",
      "Step: 104884, Return: 233.90, Len: 648\n",
      "Step: 105531, Return: 234.75, Len: 647\n",
      "Step: 105968, Return: 206.72, Len: 437\n",
      "Step: 106416, Return: 253.36, Len: 448\n",
      "Step: 107114, Return: 183.13, Len: 698\n",
      "Step: 107394, Return: 267.43, Len: 280\n",
      "Step: 107770, Return: 220.67, Len: 376\n",
      "Step: 108675, Return: 137.72, Len: 905\n",
      "Step: 108839, Return: 232.25, Len: 164\n",
      "Step: 109316, Return: 212.55, Len: 477\n",
      "Step: 109839, Return: 219.71, Len: 523\n",
      "Step: 110258, Return: 238.96, Len: 419\n",
      "Step: 110602, Return: 223.64, Len: 344\n",
      "Step: 110767, Return: 289.65, Len: 165\n",
      "Step: 110990, Return: 250.07, Len: 223\n",
      "Step: 111443, Return: 181.53, Len: 453\n",
      "Step: 111774, Return: 267.59, Len: 331\n",
      "Step: 112085, Return: -91.63, Len: 311\n",
      "Step: 112555, Return: 277.64, Len: 470\n",
      "Step: 113244, Return: 200.11, Len: 689\n",
      "Step: 113678, Return: 273.30, Len: 434\n",
      "Step: 113976, Return: 231.26, Len: 298\n",
      "Step: 114080, Return: 32.26, Len: 104\n",
      "Step: 114249, Return: 262.53, Len: 169\n",
      "Step: 114507, Return: 269.88, Len: 258\n",
      "Step: 114720, Return: 237.52, Len: 213\n",
      "Step: 115039, Return: 226.93, Len: 319\n",
      "Step: 116022, Return: 117.87, Len: 983\n",
      "Step: 116311, Return: 229.93, Len: 289\n",
      "Step: 116794, Return: 212.06, Len: 483\n",
      "Step: 117215, Return: 237.23, Len: 421\n",
      "Step: 117412, Return: 268.76, Len: 197\n",
      "Step: 117572, Return: -12.60, Len: 160\n",
      "Step: 117823, Return: 279.72, Len: 251\n",
      "Step: 118041, Return: 263.34, Len: 218\n",
      "Step: 118247, Return: 238.23, Len: 206\n",
      "Step: 118362, Return: 29.37, Len: 115\n",
      "Step: 118579, Return: 263.31, Len: 217\n",
      "Step: 118767, Return: 269.48, Len: 188\n",
      "Step: 118952, Return: 292.92, Len: 185\n",
      "Step: 119093, Return: 284.28, Len: 141\n",
      "Step: 119432, Return: 232.16, Len: 339\n",
      "Step: 119592, Return: 241.09, Len: 160\n",
      "Step: 119930, Return: 230.57, Len: 338\n",
      "Step: 120145, Return: 276.59, Len: 215\n",
      "Step: 120458, Return: 254.48, Len: 313\n",
      "Step: 120571, Return: 30.88, Len: 113\n",
      "Step: 120735, Return: 242.48, Len: 164\n",
      "Step: 120911, Return: 44.68, Len: 176\n",
      "Step: 121523, Return: 264.30, Len: 612\n",
      "Step: 121597, Return: -0.38, Len: 74\n",
      "Step: 121712, Return: 33.47, Len: 115\n",
      "Step: 121815, Return: -40.51, Len: 103\n",
      "Step: 121973, Return: 225.63, Len: 158\n",
      "Step: 122268, Return: 274.32, Len: 295\n",
      "Step: 122871, Return: -168.61, Len: 603\n",
      "Step: 123189, Return: -206.67, Len: 318\n",
      "Step: 123482, Return: 256.74, Len: 293\n",
      "Step: 123632, Return: -1.98, Len: 150\n",
      "Step: 124051, Return: -191.37, Len: 419\n",
      "Step: 124248, Return: 250.13, Len: 197\n",
      "Step: 124576, Return: 261.53, Len: 328\n",
      "Step: 125174, Return: -298.46, Len: 598\n",
      "Step: 125516, Return: 238.77, Len: 342\n",
      "Step: 125867, Return: 265.31, Len: 351\n",
      "Step: 126220, Return: 246.90, Len: 353\n",
      "Step: 126481, Return: 221.27, Len: 261\n",
      "Step: 126840, Return: 184.42, Len: 359\n",
      "Step: 127142, Return: 252.84, Len: 302\n",
      "Step: 127431, Return: 294.14, Len: 289\n",
      "Step: 127782, Return: 241.21, Len: 351\n",
      "Step: 128772, Return: 151.45, Len: 990\n",
      "Step: 129051, Return: 253.13, Len: 279\n",
      "Step: 129336, Return: 272.30, Len: 285\n",
      "Step: 129492, Return: -129.31, Len: 156\n",
      "Step: 129775, Return: 200.35, Len: 283\n",
      "Step: 129961, Return: 238.25, Len: 186\n",
      "Step: 130199, Return: 282.88, Len: 238\n",
      "Step: 130441, Return: 277.43, Len: 242\n",
      "Step: 130946, Return: -352.07, Len: 505\n",
      "Step: 131480, Return: 165.20, Len: 534\n",
      "Step: 132021, Return: -203.58, Len: 541\n",
      "Step: 132503, Return: 222.89, Len: 482\n",
      "Step: 133057, Return: -258.92, Len: 554\n",
      "Step: 133280, Return: 264.66, Len: 223\n",
      "Step: 133561, Return: 241.69, Len: 281\n",
      "Step: 133820, Return: 242.44, Len: 259\n",
      "Step: 134021, Return: 254.65, Len: 201\n",
      "Step: 134251, Return: 231.23, Len: 230\n",
      "Step: 134810, Return: 214.08, Len: 559\n",
      "Step: 135161, Return: 230.01, Len: 351\n",
      "Step: 135430, Return: 258.04, Len: 269\n",
      "Step: 135814, Return: -202.67, Len: 384\n",
      "Step: 136066, Return: 240.28, Len: 252\n",
      "Step: 136545, Return: 188.66, Len: 479\n",
      "Step: 136833, Return: 238.10, Len: 288\n",
      "Step: 136986, Return: 271.30, Len: 153\n",
      "Step: 137459, Return: -136.00, Len: 473\n",
      "Step: 137828, Return: 262.22, Len: 369\n",
      "Step: 138028, Return: 286.24, Len: 200\n",
      "Step: 138628, Return: -291.34, Len: 600\n",
      "Step: 139022, Return: 202.11, Len: 394\n",
      "Step: 139366, Return: 227.04, Len: 344\n",
      "Step: 139465, Return: 32.69, Len: 99\n",
      "Step: 139589, Return: -18.57, Len: 124\n",
      "Step: 139841, Return: 264.08, Len: 252\n",
      "Step: 140050, Return: 232.36, Len: 209\n",
      "Step: 140739, Return: -169.38, Len: 689\n",
      "Step: 140932, Return: 258.00, Len: 193\n",
      "Step: 141168, Return: 274.78, Len: 236\n",
      "Step: 141363, Return: -169.01, Len: 195\n",
      "Step: 141605, Return: 212.82, Len: 242\n",
      "Step: 141966, Return: 271.47, Len: 361\n",
      "Step: 142208, Return: 252.42, Len: 242\n",
      "Step: 142463, Return: 212.60, Len: 255\n",
      "Step: 142665, Return: 276.49, Len: 202\n",
      "Step: 142938, Return: 237.68, Len: 273\n",
      "Step: 143175, Return: 276.26, Len: 237\n",
      "Step: 143840, Return: 155.11, Len: 665\n",
      "Step: 144784, Return: -209.67, Len: 944\n",
      "Step: 145041, Return: 247.89, Len: 257\n",
      "Step: 145465, Return: -490.15, Len: 424\n",
      "Step: 145713, Return: 243.25, Len: 248\n",
      "Step: 145928, Return: 244.87, Len: 215\n",
      "Step: 146247, Return: 244.53, Len: 319\n",
      "Step: 146855, Return: 167.84, Len: 608\n",
      "Step: 147080, Return: 266.81, Len: 225\n",
      "Step: 147274, Return: 286.02, Len: 194\n",
      "Step: 147501, Return: 268.34, Len: 227\n",
      "Step: 148501, Return: -94.36, Len: 1000\n",
      "Step: 148703, Return: 36.25, Len: 202\n",
      "Step: 148965, Return: 248.50, Len: 262\n",
      "Step: 149120, Return: 235.22, Len: 155\n",
      "Step: 149320, Return: 243.94, Len: 200\n",
      "Step: 149567, Return: 253.73, Len: 247\n",
      "Step: 150043, Return: -250.23, Len: 476\n",
      "Step: 150614, Return: 174.51, Len: 571\n",
      "Step: 150915, Return: 227.43, Len: 301\n",
      "Step: 151174, Return: 239.56, Len: 259\n",
      "Step: 151363, Return: 215.62, Len: 189\n",
      "Step: 151610, Return: 242.61, Len: 247\n",
      "Step: 152124, Return: -137.88, Len: 514\n",
      "Step: 152845, Return: -98.77, Len: 721\n",
      "Step: 153641, Return: -384.97, Len: 796\n",
      "Step: 153884, Return: 271.25, Len: 243\n",
      "Step: 154119, Return: 241.04, Len: 235\n",
      "Step: 154467, Return: 265.61, Len: 348\n",
      "Step: 155203, Return: 153.08, Len: 736\n",
      "Step: 155618, Return: 231.76, Len: 415\n",
      "Step: 155925, Return: 212.25, Len: 307\n",
      "Step: 156168, Return: 249.38, Len: 243\n",
      "Step: 156588, Return: 205.70, Len: 420\n",
      "Step: 156773, Return: 260.87, Len: 185\n",
      "Step: 157037, Return: 234.41, Len: 264\n",
      "Step: 157417, Return: 145.00, Len: 380\n",
      "Step: 157532, Return: 12.29, Len: 115\n",
      "Step: 157746, Return: 244.19, Len: 214\n",
      "Step: 157988, Return: 200.19, Len: 242\n",
      "Step: 158218, Return: 233.71, Len: 230\n",
      "Step: 158420, Return: 245.68, Len: 202\n",
      "Step: 158654, Return: 250.56, Len: 234\n",
      "Step: 158889, Return: 278.67, Len: 235\n",
      "Step: 159513, Return: 177.36, Len: 624\n",
      "Step: 159715, Return: 272.34, Len: 202\n",
      "Step: 160045, Return: 264.34, Len: 330\n",
      "Step: 160247, Return: 257.29, Len: 202\n",
      "Step: 160398, Return: 241.83, Len: 151\n",
      "Step: 160630, Return: 253.40, Len: 232\n",
      "Step: 160812, Return: 267.36, Len: 182\n",
      "Step: 161076, Return: 226.99, Len: 264\n",
      "Step: 161879, Return: 90.36, Len: 803\n",
      "Step: 162073, Return: 280.52, Len: 194\n",
      "Step: 162252, Return: 267.45, Len: 179\n",
      "Step: 162429, Return: 223.77, Len: 177\n",
      "Step: 162525, Return: -1.47, Len: 96\n",
      "Step: 163525, Return: -5.97, Len: 1000\n",
      "Step: 163719, Return: 259.86, Len: 194\n",
      "Step: 163862, Return: 264.81, Len: 143\n",
      "Step: 164188, Return: 241.91, Len: 326\n",
      "Step: 164455, Return: 234.25, Len: 267\n",
      "Step: 165455, Return: -53.77, Len: 1000\n",
      "Step: 165603, Return: 280.29, Len: 148\n",
      "Step: 166238, Return: 138.60, Len: 635\n",
      "Step: 166480, Return: 241.78, Len: 242\n",
      "Step: 166938, Return: 263.17, Len: 458\n",
      "Step: 167111, Return: 274.83, Len: 173\n",
      "Step: 167359, Return: 261.47, Len: 248\n",
      "Step: 167921, Return: 163.07, Len: 562\n",
      "Step: 168126, Return: 263.37, Len: 205\n",
      "Step: 168486, Return: 219.16, Len: 360\n",
      "Step: 168711, Return: 245.58, Len: 225\n",
      "Step: 168911, Return: 258.48, Len: 200\n",
      "Step: 169094, Return: 282.14, Len: 183\n",
      "Step: 169308, Return: 246.88, Len: 214\n",
      "Step: 169798, Return: -127.56, Len: 490\n",
      "Step: 169977, Return: 265.88, Len: 179\n",
      "Step: 170097, Return: 26.10, Len: 120\n",
      "Step: 170259, Return: 266.71, Len: 162\n",
      "Step: 170517, Return: 283.45, Len: 258\n",
      "Step: 170728, Return: 271.34, Len: 211\n",
      "Step: 170901, Return: 264.99, Len: 173\n",
      "Step: 171105, Return: 228.05, Len: 204\n",
      "Step: 172105, Return: 32.30, Len: 1000\n",
      "Step: 172283, Return: 272.86, Len: 178\n",
      "Step: 172622, Return: 249.95, Len: 339\n",
      "Step: 173622, Return: 4.54, Len: 1000\n",
      "Step: 174204, Return: 144.76, Len: 582\n",
      "Step: 175156, Return: 102.25, Len: 952\n",
      "Step: 175370, Return: 189.21, Len: 214\n",
      "Step: 175617, Return: 262.73, Len: 247\n",
      "Step: 176167, Return: 116.57, Len: 550\n",
      "Step: 176375, Return: 273.70, Len: 208\n",
      "Step: 176594, Return: 20.28, Len: 219\n",
      "Step: 176782, Return: 244.91, Len: 188\n",
      "Step: 176985, Return: 254.98, Len: 203\n",
      "Step: 177522, Return: 149.08, Len: 537\n",
      "Step: 178025, Return: -283.83, Len: 503\n",
      "Step: 178226, Return: 265.26, Len: 201\n",
      "Step: 178531, Return: 298.39, Len: 305\n",
      "Step: 178973, Return: -57.67, Len: 442\n",
      "Step: 179776, Return: 119.03, Len: 803\n",
      "Step: 179905, Return: 51.13, Len: 129\n",
      "Step: 180061, Return: 281.28, Len: 156\n",
      "Step: 180263, Return: 270.03, Len: 202\n",
      "Step: 180439, Return: 252.74, Len: 176\n",
      "Step: 181216, Return: -229.51, Len: 777\n",
      "Step: 182216, Return: -10.57, Len: 1000\n",
      "Step: 182389, Return: 254.33, Len: 173\n",
      "Step: 182575, Return: 298.20, Len: 186\n",
      "Step: 182891, Return: 251.87, Len: 316\n",
      "Step: 183041, Return: 282.87, Len: 150\n",
      "Step: 183184, Return: 267.83, Len: 143\n",
      "Step: 183374, Return: 238.70, Len: 190\n",
      "Step: 183732, Return: -303.40, Len: 358\n",
      "Step: 183888, Return: 277.98, Len: 156\n",
      "Step: 184089, Return: 226.07, Len: 201\n",
      "Step: 184188, Return: -20.88, Len: 99\n",
      "Step: 184369, Return: 259.29, Len: 181\n",
      "Step: 184539, Return: 287.98, Len: 170\n",
      "Step: 184645, Return: -55.64, Len: 106\n",
      "Step: 184814, Return: 262.75, Len: 169\n",
      "Step: 184982, Return: 274.35, Len: 168\n",
      "Step: 185150, Return: 264.21, Len: 168\n",
      "Step: 185296, Return: 281.76, Len: 146\n",
      "Step: 185638, Return: 239.49, Len: 342\n",
      "Step: 185951, Return: 232.71, Len: 313\n",
      "Step: 186131, Return: 256.08, Len: 180\n",
      "Step: 186297, Return: 275.00, Len: 166\n",
      "Step: 186475, Return: 240.02, Len: 178\n",
      "Step: 186661, Return: 270.74, Len: 186\n",
      "Step: 187661, Return: -60.65, Len: 1000\n",
      "Step: 187873, Return: 15.74, Len: 212\n",
      "Step: 188218, Return: 215.00, Len: 345\n",
      "Step: 188437, Return: 280.87, Len: 219\n",
      "Step: 188685, Return: 247.92, Len: 248\n",
      "Step: 188849, Return: 271.17, Len: 164\n",
      "Step: 189047, Return: 248.55, Len: 198\n",
      "Step: 190047, Return: 28.30, Len: 1000\n",
      "Step: 190254, Return: 285.37, Len: 207\n",
      "Step: 190438, Return: 276.11, Len: 184\n",
      "Step: 190607, Return: 279.50, Len: 169\n",
      "Step: 190751, Return: 290.69, Len: 144\n",
      "Step: 190984, Return: 211.34, Len: 233\n",
      "Step: 191153, Return: 266.54, Len: 169\n",
      "Step: 191609, Return: 208.58, Len: 456\n",
      "Step: 191786, Return: -0.37, Len: 177\n",
      "Step: 192786, Return: -33.83, Len: 1000\n",
      "Step: 192977, Return: 256.03, Len: 191\n",
      "Step: 193383, Return: 219.74, Len: 406\n",
      "Step: 193607, Return: 275.74, Len: 224\n",
      "Step: 193792, Return: 247.20, Len: 185\n",
      "Step: 194002, Return: 224.24, Len: 210\n",
      "Step: 194177, Return: 253.32, Len: 175\n",
      "Step: 194381, Return: 273.88, Len: 204\n",
      "Step: 194565, Return: 254.24, Len: 184\n",
      "Step: 195565, Return: 8.10, Len: 1000\n",
      "Step: 195662, Return: -9.39, Len: 97\n",
      "Step: 195821, Return: 239.78, Len: 159\n",
      "Step: 196090, Return: 211.87, Len: 269\n",
      "Step: 196164, Return: 26.44, Len: 74\n",
      "Step: 197164, Return: -7.06, Len: 1000\n",
      "Step: 197351, Return: 269.86, Len: 187\n",
      "Step: 197547, Return: 224.19, Len: 196\n",
      "Step: 197737, Return: 217.78, Len: 190\n",
      "Step: 198737, Return: -16.57, Len: 1000\n",
      "Step: 198917, Return: 250.15, Len: 180\n",
      "Step: 199128, Return: 233.81, Len: 211\n",
      "Step: 199332, Return: 259.98, Len: 204\n",
      "Step: 199473, Return: 29.70, Len: 141\n",
      "Step: 199703, Return: 268.94, Len: 230\n",
      "Step: 199904, Return: 265.11, Len: 201\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPTR5ML/V2UYQLTGvLXptw2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
